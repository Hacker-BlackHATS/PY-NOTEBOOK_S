{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOn3xbenUlQGs7YSFn6wd0d"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This dataset consists of 4095 images belonging to two classes:  \n","* with_mask: 2165 images\n","* without_mask: 1930 images  \n","The images used were real images of faces wearing masks. The images were collected from the following sources:\n","\n","* Bing Search API (See Python script)\n","* Kaggle datasets\n","* RMFD dataset\n","\n","**Dependencies and required Libraries**\n","* tensorflow>=2.5.0*\n","* keras==2.4.3\n","* imutils==0.5.4\n","* numpy==1.19.5\n","* opencv-python>=4.2.0.32\n","* matplotlib==3.4.1\n","* argparse==1.4.0\n","* scipy==1.6.2\n","* scikit-learn==0.24.1\n","* pillow>=8.3.2\n","* streamlit==0.79.0\n","* onnx==1.10.1\n","* tf2onnx==1.9.3"],"metadata":{"id":"OnfW3UwTRkQy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-AOB2tAuQEr3"},"outputs":[],"source":["# USAGE\n","# python detect_mask_image.py --image images/pic1.jpeg\n","\n","# import the necessary packages\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.models import load_model\n","import numpy as np\n","import argparse\n","import cv2\n","import os\n","def mask_image():\n","\t# construct the argument parser and parse the arguments\n","\tap = argparse.ArgumentParser()\n","\tap.add_argument(\"-i\", \"--image\", required=True,\n","\t\thelp=\"path to input image\")\n","\tap.add_argument(\"-f\", \"--face\", type=str,\n","\t\tdefault=\"face_detector\",\n","\t\thelp=\"path to face detector model directory\")\n","\tap.add_argument(\"-m\", \"--model\", type=str,\n","\t\tdefault=\"mask_detector.model\",\n","\t\thelp=\"path to trained face mask detector model\")\n","\tap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n","\t\thelp=\"minimum probability to filter weak detections\")\n","\targs = vars(ap.parse_args())\n","\n","\t# load our serialized face detector model from disk\n","\tprint(\"[INFO] loading face detector model...\")\n","\tprototxtPath = os.path.sep.join([args[\"face\"], \"deploy.prototxt\"])\n","\tweightsPath = os.path.sep.join([args[\"face\"],\n","\t\t\"res10_300x300_ssd_iter_140000.caffemodel\"])\n","\tnet = cv2.dnn.readNet(prototxtPath, weightsPath)\n","\n","\t# load the face mask detector model from disk\n","\tprint(\"[INFO] loading face mask detector model...\")\n","\tmodel = load_model(args[\"model\"])\n","\n","\t# load the input image from disk, clone it, and grab the image spatial\n","\t# dimensions\n","\timage = cv2.imread(args[\"image\"])\n","\torig = image.copy()\n","\t(h, w) = image.shape[:2]\n","\n","\t# construct a blob from the image\n","\tblob = cv2.dnn.blobFromImage(image, 1.0, (300, 300),\n","\t\t(104.0, 177.0, 123.0))\n","\n","\t# pass the blob through the network and obtain the face detections\n","\tprint(\"[INFO] computing face detections...\")\n","\tnet.setInput(blob)\n","\tdetections = net.forward()\n","\n","\t# loop over the detections\n","\tfor i in range(0, detections.shape[2]):\n","\t\t# extract the confidence (i.e., probability) associated with\n","\t\t# the detection\n","\t\tconfidence = detections[0, 0, i, 2]\n","\n","\t\t# filter out weak detections by ensuring the confidence is\n","\t\t# greater than the minimum confidence\n","\t\tif confidence > args[\"confidence\"]:\n","\t\t\t# compute the (x, y)-coordinates of the bounding box for\n","\t\t\t# the object\n","\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n","\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n","\n","\t\t\t# ensure the bounding boxes fall within the dimensions of\n","\t\t\t# the frame\n","\t\t\t(startX, startY) = (max(0, startX), max(0, startY))\n","\t\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n","\n","\t\t\t# extract the face ROI, convert it from BGR to RGB channel\n","\t\t\t# ordering, resize it to 224x224, and preprocess it\n","\t\t\tface = image[startY:endY, startX:endX]\n","\t\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n","\t\t\tface = cv2.resize(face, (224, 224))\n","\t\t\tface = img_to_array(face)\n","\t\t\tface = preprocess_input(face)\n","\t\t\tface = np.expand_dims(face, axis=0)\n","\n","\t\t\t# pass the face through the model to determine if the face\n","\t\t\t# has a mask or not\n","\t\t\t(mask, withoutMask) = model.predict(face)[0]\n","\n","\t\t\t# determine the class label and color we'll use to draw\n","\t\t\t# the bounding box and text\n","\t\t\tlabel = \"Mask\" if mask > withoutMask else \"No Mask\"\n","\t\t\tcolor = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n","\n","\t\t\t# include the probability in the label\n","\t\t\tlabel = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n","\n","\t\t\t# display the label and bounding box rectangle on the output\n","\t\t\t# frame\n","\t\t\tcv2.putText(image, label, (startX, startY - 10),\n","\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n","\t\t\tcv2.rectangle(image, (startX, startY), (endX, endY), color, 2)\n","\n","\t# show the output image\n","\tcv2.imshow(\"Output\", image)\n","\tcv2.waitKey(0)\n","\t\n","if __name__ == \"__main__\":\n","\tmask_image()"]},{"cell_type":"code","source":["# USAGE\n","# python detect_mask_video.py\n","\n","# import the necessary packages\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.models import load_model\n","from imutils.video import VideoStream\n","import numpy as np\n","import argparse\n","import imutils\n","import time\n","import cv2\n","import os\n","\n","def detect_and_predict_mask(frame, faceNet, maskNet):\n","\t# grab the dimensions of the frame and then construct a blob\n","\t# from it\n","\t(h, w) = frame.shape[:2]\n","\tblob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),\n","\t\t(104.0, 177.0, 123.0))\n","\n","\t# pass the blob through the network and obtain the face detections\n","\tfaceNet.setInput(blob)\n","\tdetections = faceNet.forward()\n","\n","\t# initialize our list of faces, their corresponding locations,\n","\t# and the list of predictions from our face mask network\n","\tfaces = []\n","\tlocs = []\n","\tpreds = []\n","\n","\t# loop over the detections\n","\tfor i in range(0, detections.shape[2]):\n","\t\t# extract the confidence (i.e., probability) associated with\n","\t\t# the detection\n","\t\tconfidence = detections[0, 0, i, 2]\n","\n","\t\t# filter out weak detections by ensuring the confidence is\n","\t\t# greater than the minimum confidence\n","\t\tif confidence > args[\"confidence\"]:\n","\t\t\t# compute the (x, y)-coordinates of the bounding box for\n","\t\t\t# the object\n","\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n","\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n","\n","\t\t\t# ensure the bounding boxes fall within the dimensions of\n","\t\t\t# the frame\n","\t\t\t(startX, startY) = (max(0, startX), max(0, startY))\n","\t\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n","\n","\t\t\t# extract the face ROI, convert it from BGR to RGB channel\n","\t\t\t# ordering, resize it to 224x224, and preprocess it\n","\t\t\tface = frame[startY:endY, startX:endX]\n","\t\t\tif face.any():\n","\t\t\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n","\t\t\t\tface = cv2.resize(face, (224, 224))\n","\t\t\t\tface = img_to_array(face)\n","\t\t\t\tface = preprocess_input(face)\n","\n","\t\t\t\t# add the face and bounding boxes to their respective\n","\t\t\t\t# lists\n","\t\t\t\tfaces.append(face)\n","\t\t\t\tlocs.append((startX, startY, endX, endY))\n","\n","\t# only make a predictions if at least one face was detected\n","\tif len(faces) > 0:\n","\t\t# for faster inference we'll make batch predictions on *all*\n","\t\t# faces at the same time rather than one-by-one predictions\n","\t\t# in the above `for` loop\n","\t\tfaces = np.array(faces, dtype=\"float32\")\n","\t\tpreds = maskNet.predict(faces, batch_size=32)\n","\n","\t# return a 2-tuple of the face locations and their corresponding\n","\t# locations\n","\treturn (locs, preds)\n","\n","# construct the argument parser and parse the arguments\n","ap = argparse.ArgumentParser()\n","ap.add_argument(\"-f\", \"--face\", type=str,\n","\tdefault=\"face_detector\",\n","\thelp=\"path to face detector model directory\")\n","ap.add_argument(\"-m\", \"--model\", type=str,\n","\tdefault=\"mask_detector.model\",\n","\thelp=\"path to trained face mask detector model\")\n","ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n","\thelp=\"minimum probability to filter weak detections\")\n","args = vars(ap.parse_args())\n","\n","# load our serialized face detector model from disk\n","print(\"[INFO] loading face detector model...\")\n","prototxtPath = os.path.sep.join([args[\"face\"], \"deploy.prototxt\"])\n","weightsPath = os.path.sep.join([args[\"face\"],\n","\t\"res10_300x300_ssd_iter_140000.caffemodel\"])\n","faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n","\n","# load the face mask detector model from disk\n","print(\"[INFO] loading face mask detector model...\")\n","maskNet = load_model(args[\"model\"])\n","\n","# initialize the video stream and allow the camera sensor to warm up\n","print(\"[INFO] starting video stream...\")\n","vs = VideoStream(src=0).start()\n","time.sleep(2.0)\n","\n","# loop over the frames from the video stream\n","while True:\n","\t# grab the frame from the threaded video stream and resize it\n","\t# to have a maximum width of 400 pixels\n","\tframe = vs.read()\n","\tframe = imutils.resize(frame, width=400)\n","\n","\t# detect faces in the frame and determine if they are wearing a\n","\t# face mask or not\n","\t(locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)\n","\n","\t# loop over the detected face locations and their corresponding\n","\t# locations\n","\tfor (box, pred) in zip(locs, preds):\n","\t\t# unpack the bounding box and predictions\n","\t\t(startX, startY, endX, endY) = box\n","\t\t(mask, withoutMask) = pred\n","\n","\t\t# determine the class label and color we'll use to draw\n","\t\t# the bounding box and text\n","\t\tlabel = \"Mask\" if mask > withoutMask else \"No Mask\"\n","\t\tcolor = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n","\t\t\t\n","\t\t# include the probability in the label\n","\t\tlabel = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n","\n","\t\t# display the label and bounding box rectangle on the output\n","\t\t# frame\n","\t\tcv2.putText(frame, label, (startX, startY - 10),\n","\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n","\t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n","\n","\t# show the output frame\n","\tcv2.imshow(\"Frame\", frame)\n","\tkey = cv2.waitKey(1) & 0xFF\n","\n","\t# if the `q` key was pressed, break from the loop\n","\tif key == ord(\"q\"):\n","\t\tbreak\n","\n","# do a bit of cleanup\n","cv2.destroyAllWindows()\n","vs.stop()"],"metadata":{"id":"v2Q2K4oicjgj"},"execution_count":null,"outputs":[]}]}